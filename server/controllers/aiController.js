const { GoogleGenerativeAI } = require('@google/generative-ai');

// Define AI model personas - all powered by Gemini
const AI_PERSONAS = {
    chatgpt: {
        name: 'ChatGPT (OpenAI)',
        systemPrompt: `You are ChatGPT, a large language model trained by OpenAI. 
You are helpful, creative, clever, and very friendly. You provide detailed, well-structured responses.
You tend to be conversational and sometimes use phrases like "I'm happy to help" or "Let me explain".
Format your responses in a clear, organized manner with proper paragraphs.`
    },
    claude: {
        name: 'Claude (Anthropic)',
        systemPrompt: `You are Claude, an AI assistant created by Anthropic.
You are thoughtful, nuanced, and aim to be helpful, harmless, and honest.
You provide balanced perspectives and often acknowledge complexity in topics.
You're known for being detailed and thorough in your explanations.
You sometimes use phrases like "I appreciate your question" or "Let me think through this carefully".`
    },
    gemini: {
        name: 'Gemini (Google)',
        systemPrompt: `You are Gemini, Google's most capable AI model.
You are direct, informative, and precise. You provide comprehensive answers backed by knowledge.
You excel at breaking down complex topics and providing clear explanations.
You maintain a professional yet approachable tone.`
    }
};

// Helper function to call Gemini with a specific persona
const callGeminiWithPersona = async (prompt, modelKey) => {
    if (!process.env.MY_GEMINI_KEY) {
        return 'Configuration Error: Gemini API Key missing.';
    }

    const persona = AI_PERSONAS[modelKey];
    if (!persona) {
        return 'Error: Unknown AI model';
    }

    try {
        const genAI = new GoogleGenerativeAI(process.env.MY_GEMINI_KEY);
        const model = genAI.getGenerativeModel({ model: "gemini-2.5-flash" });

        // Combine system prompt with user prompt
        const fullPrompt = `${persona.systemPrompt}\n\nUser Query: ${prompt}\n\nRespond in character as ${persona.name}:`;

        const result = await model.generateContent(fullPrompt);
        const response = await result.response;
        return response.text();
    } catch (error) {
        throw new Error(`${persona.name} Failed: ${error.message}`);
    }
};

// Get responses from all AI models
const getResponses = async (req, res) => {
    const { prompt } = req.body;

    if (!prompt) {
        return res.status(400).json({ error: 'Prompt is required' });
    }

    try {
        // Call all models in parallel
        const responses = await Promise.allSettled([
            callGeminiWithPersona(prompt, 'chatgpt'),
            callGeminiWithPersona(prompt, 'claude'),
            callGeminiWithPersona(prompt, 'gemini')
        ]);

        // Extract values or errors
        const [chatgptRes, claudeRes, geminiRes] = responses.map(r =>
            r.status === 'fulfilled' ? r.value : `Error: ${r.reason.message}`
        );

        res.json({
            chatgpt: chatgptRes,
            claude: claudeRes,
            gemini: geminiRes
        });
    } catch (error) {
        console.error('Error fetching AI responses:', error);
        res.status(500).json({ error: 'Failed to fetch responses' });
    }
};

// Streaming endpoint with personas
const streamResponses = async (req, res) => {
    const { prompt } = req.body;

    if (!prompt) {
        return res.status(400).json({ error: 'Prompt is required' });
    }

    // Set headers for Server-Sent Events
    res.setHeader('Content-Type', 'text/event-stream');
    res.setHeader('Cache-Control', 'no-cache');
    res.setHeader('Connection', 'keep-alive');

    // Function to send SSE messages
    const sendEvent = (model, data, done = false) => {
        res.write(`data: ${JSON.stringify({ model, data, done })}\n\n`);
    };

    try {
        // Stream all models in parallel
        await Promise.all([
            streamModelWithPersona(prompt, 'chatgpt', sendEvent),
            streamModelWithPersona(prompt, 'claude', sendEvent),
            streamModelWithPersona(prompt, 'gemini', sendEvent)
        ]);

        res.end();
    } catch (error) {
        console.error('Streaming error:', error);
        res.end();
    }
};

// Stream Gemini with persona
const streamModelWithPersona = async (prompt, modelKey, sendEvent) => {
    if (!process.env.MY_GEMINI_KEY) {
        sendEvent(modelKey, 'Configuration Error: Gemini API Key missing.', true);
        return;
    }

    const persona = AI_PERSONAS[modelKey];
    if (!persona) {
        sendEvent(modelKey, 'Error: Unknown AI model', true);
        return;
    }

    try {
        const genAI = new GoogleGenerativeAI(process.env.MY_GEMINI_KEY);
        const model = genAI.getGenerativeModel({ model: "gemini-2.5-flash" });

        // Combine system prompt with user prompt
        const fullPrompt = `${persona.systemPrompt}\n\nUser Query: ${prompt}\n\nRespond in character as ${persona.name}:`;

        const result = await model.generateContentStream(fullPrompt);

        for await (const chunk of result.stream) {
            const chunkText = chunk.text();
            if (chunkText) {
                sendEvent(modelKey, chunkText, false);
            }
        }

        sendEvent(modelKey, '', true); // Signal completion
    } catch (error) {
        sendEvent(modelKey, `Error: ${error.message}`, true);
    }
};

// Summarize all responses
const summarizeResponses = async (req, res) => {
    console.log('ðŸ“ Summarize request received');
    const { responses } = req.body;

    console.log('Received responses:', Object.keys(responses || {}));

    if (!responses || typeof responses !== 'object') {
        console.log('âŒ Invalid responses object');
        return res.status(400).json({ error: 'Responses object is required' });
    }

    try {
        // Build a simple, clear prompt with all AI responses
        const responsesText = Object.entries(responses)
            .filter(([_, response]) => response && response.trim() !== '')
            .map(([model, response]) => `\n### ${model.toUpperCase()} Response:\n${response}`)
            .join('\n\n---\n');

        const summaryPrompt = `You are an expert analyst. I have received responses from multiple AI models (ChatGPT, Claude, and Gemini) to the same question.

Please analyze all the responses below and create a comprehensive summary that:
1. Captures all the important information and key insights from each AI model
2. Identifies common themes and agreements across different models
3. Highlights unique perspectives or information that only specific models mentioned
4. Synthesizes everything into a clear, well-organized, and complete answer
5. Uses bullet points and clear formatting for readability

Here are all the AI responses:
${responsesText}

Now, provide a comprehensive summary that combines all the important points from these AI responses:`;

        if (!process.env.MY_GEMINI_KEY) {
            console.log('âŒ Gemini API Key missing');
            return res.status(500).json({ error: 'Gemini API Key missing' });
        }

        console.log('ðŸ¤– Calling Gemini for summary...');
        const genAI = new GoogleGenerativeAI(process.env.MY_GEMINI_KEY);
        const model = genAI.getGenerativeModel({ model: "gemini-2.5-flash" });

        const result = await model.generateContent(summaryPrompt);
        const response = await result.response;
        const summary = response.text();

        console.log('âœ… Summary generated successfully');
        res.json({ summary });
    } catch (error) {
        console.error('âŒ Summarization error:', error);
        res.status(500).json({ error: 'Failed to generate summary', details: error.message });
    }
};

// Chat endpoint for Mock Interview
const chat = async (req, res) => {
    const { messages, jobRole } = req.body;

    console.log(`ðŸ’¬ Chat request for role: ${jobRole}, msg count: ${messages?.length}`);

    if (!messages || !Array.isArray(messages)) {
        return res.status(400).json({ error: 'Messages array is required' });
    }

    try {
        if (!process.env.MY_GEMINI_KEY) {
            throw new Error("Gemini API Key is missing in environment variables.");
        }

        const genAI = new GoogleGenerativeAI(process.env.MY_GEMINI_KEY);
        // Use gemini-2.5-flash as default, fallback if needed
        const model = genAI.getGenerativeModel({
            model: "gemini-2.5-flash",
            systemInstruction: `You are an expert technical interviewer for a ${jobRole} position.
            Conduct a professional interview. Ask one relevant question at a time.
            Evaluate the user's answers briefly before moving to the next question.
            Keep responses concise and conversational.`
        });

        // Current message to send (the last one from user)
        const lastMessage = messages[messages.length - 1];
        if (lastMessage.sender !== 'user') {
            return res.status(400).json({ error: 'Last message must be from user' });
        }

        // History: Filter out the very first message if it's from AI (the initial greeting)
        // AND strip the last message (which is the current prompt)
        const historyMessages = messages.slice(0, -1).filter((m, index) => {
            // Remove first message if it's AI greeting
            if (index === 0 && m.sender === 'ai') return false;
            return true;
        });

        const history = historyMessages.map(m => ({
            role: m.sender === 'user' ? 'user' : 'model',
            parts: [{ text: m.text }]
        }));

        console.log("ðŸ“œ Chat History for Gemini:", JSON.stringify(history, null, 2));

        const chatSession = model.startChat({
            history: history,
        });

        const result = await chatSession.sendMessage(lastMessage.text);
        const response = await result.response;

        // Safety check
        if (!response.candidates || response.candidates.length === 0) {
            throw new Error("No response candidates returned. Possibly blocked.");
        }

        const text = response.text();

        res.json({ text });
    } catch (error) {
        console.error('Chat error details:', JSON.stringify(error, Object.getOwnPropertyNames(error)));
        res.status(500).json({ error: 'Chat failed', details: error.message || error.toString() });
    }
};

module.exports = {
    getResponses,
    streamResponses,
    summarizeResponses,
    chat
};
